{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6adb83c-f9d9-484c-94ab-5096e53069b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import average_precision_score\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1506f2-dfdb-4e5c-a551-b603cc4f3fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brands = os.listdir(\"./action_effect_image_rs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3774f0fb-037d-4b92-a4c0-763189de8cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brands = [b.split(\"+\") for b in brands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc9df3e-6459-492b-9a14-d6dffcec862c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = {b[0] + \"+\" + b[1]: b for b in brands}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c2ae09-1b49-4030-a5a2-7bf4132821cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"results.json\") as f:\n",
    "    results = json.load(f)\n",
    "    metrics0 = precision_recall_fscore_support(results[\"label\"], results[\"preds\"], average=\"micro\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff323e31-8c3f-423c-ada0-da1f8ca33a02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41748046875, 0.41748046875, 0.41748046875, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97eaad5-e14f-488c-9ac7-45b4a48db171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianlix/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "with open(\"results.json\") as f:\n",
    "    results = json.load(f)\n",
    "    metrics1 = precision_recall_fscore_support(results[\"label\"], results[\"preds\"], average=\"macro\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6248a7-e2e2-45b0-a3b5-b33ad6e5de00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39449355235471634, 0.42414709650002325, 0.3698221443359037, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c5b8ec6-1c5f-46e9-aa60-71a92a0b395f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"results.json\") as f:\n",
    "    results = json.load(f)\n",
    "    metrics2 = accuracy_score(results[\"label\"], results[\"preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14148bd8-53ee-48e1-a0bb-dc84a58d5203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41748046875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3fe47f-a521-415c-bffb-79f2a1db7ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_one_hot(input_array, num_class):\n",
    "    out_array = np.eye(num_class)[input_array]\n",
    "    return out_array\n",
    "\n",
    "def mean_average_precision(all_probs, all_labels):\n",
    "    '''\n",
    "    Args:\n",
    "        all_probs: 2D numpy.ndarray. The first dimension is number of samples, and the second one is the number of class\n",
    "        all_labels: 1D numpy.ndarray. Save the index of labels.\n",
    "    '''\n",
    "    n_sample, num_class = all_probs.shape\n",
    "    \n",
    "    all_labels = make_one_hot(all_labels, num_class)\n",
    "    n_precision = []\n",
    "    for each_class in range(num_class):\n",
    "        probs = all_probs[:,each_class]\n",
    "        labels = all_labels[:,each_class]\n",
    "        order = np.argsort(-probs) # Sort by confidence from largest to smallest\n",
    "        probs = probs[order]\n",
    "        labels = labels[order]\n",
    "        precision = []\n",
    "        recall = []\n",
    "        for i in range(n_sample):\n",
    "            pos_pred_label = labels[0:i+1]\n",
    "            neg_pred_label = labels[i+1:]\n",
    "            tp = np.sum(pos_pred_label)\n",
    "            fp = len(pos_pred_label) - tp\n",
    "            fn = np.sum(neg_pred_label)\n",
    "            P = tp / (tp + fp + 1e-10)\n",
    "            R = tp / (tp + fn + 1e-10)\n",
    "            precision.append(P)\n",
    "            recall.append(R)\n",
    "        recall_change_index_0 = [] # The same recall value may correspond multiple precision values. So we take the largest precision value.\n",
    "        for i in range(n_sample-1):\n",
    "            if recall[i] != recall[i+1]:\n",
    "                recall_change_index_0.append(i+1)\n",
    "        recall_change_index_1 = recall_change_index_0[0:]\n",
    "        recall_change_index_0.insert(0, 0)\n",
    "        recall_change_index_1.append(n_sample)\n",
    "        precision = np.array(precision)\n",
    "        recall = np.array(recall)\n",
    "        for i in range(len(recall_change_index_1)):\n",
    "            index_0 = recall_change_index_0[i]\n",
    "            index_1 = recall_change_index_1[i]\n",
    "            precision[index_0:index_1] = np.max(precision[index_0:])\n",
    "        unique_precision = []\n",
    "        unique_precision.append(precision[0])\n",
    "        for i in range(n_sample-1):\n",
    "            if recall[i] != recall[i+1]:  # Only take precision when recall changes\n",
    "                unique_precision.append(precision[i+1])\n",
    "        n_precision.append(np.mean(unique_precision))\n",
    "    \n",
    "    mAP = np.mean(np.array(n_precision))\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac000a14-59dc-49bb-b506-22e4188ab936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "with open(\"results_topk_no_bg.json\") as f:\n",
    "    results = json.load(f)\n",
    "    label = np.array(results[\"label\"])\n",
    "    probs= np.array(results[\"probs\"])\n",
    "    metrics1 = mean_average_precision(probs, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bd126e3-cc83-47db-86c8-a78497741b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5036904481295816"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac7c00b-0295-451c-b4a5-24a4999e578b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4189453125\n",
      "0.828125\n",
      "0.9765625\n"
     ]
    }
   ],
   "source": [
    "top1, top5, top20, total = 0,0,0,0\n",
    "with open(\"results_topk_no_bg.json\") as f:\n",
    "    results = json.load(f)\n",
    "    label = results[\"label\"]\n",
    "    preds= results[\"preds\"]\n",
    "    for i, item in enumerate(label):\n",
    "        total += 1\n",
    "        if item == preds[i][0]:\n",
    "            top1 += 1\n",
    "            top5 += 1\n",
    "            top20 += 1\n",
    "        elif item in preds[i][:5]:\n",
    "            top5 += 1\n",
    "            top20 += 1\n",
    "        elif item in preds[i][:20]:\n",
    "            top20 += 1\n",
    "    print(top1/total)\n",
    "    print(top5/total)\n",
    "    print(top20/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f7e7f80-451f-48a2-9056-c4ed1b3518f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The micro precision is 0.41748046875, recall is 0.41748046875, f1 score is 0.41748046875\n"
     ]
    }
   ],
   "source": [
    "print(\"The micro precision is {}, recall is {}, f1 score is {}\".format(metrics0[0], metrics0[1], metrics0[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05d81c66-70cf-4697-95c4-fb6c269fdf5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The macro precision is 0.39449355235471634, recall is 0.42414709650002325, f1 score is 0.3698221443359037\n"
     ]
    }
   ],
   "source": [
    "print(\"The macro precision is {}, recall is {}, f1 score is {}\".format(metrics1[0], metrics1[1], metrics1[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d8bd8b7-569f-45e2-91c4-0d8a1ee85693",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 0.41748046875\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy is: {}\".format(metrics2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d5d61c3-59d0-4c18-911c-c868139ab6ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3905848/2852839199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m ])\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0maverage_precision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36maverage_precision_score\u001b[0;34m(y_true, y_score, average, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0m_binary_uninterpolated_average_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     )\n\u001b[0;32m--> 234\u001b[0;31m     return _average_binary_score(\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0maverage_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "average_precision_score(y_true, y_scores)\n",
    "y_true = np.array([0, 0, 1, 1, 2, 2])\n",
    "y_scores = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.4, 0.3, 0.3],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.2, 0.3, 0.5],\n",
    "    [0.4, 0.4, 0.2],\n",
    "    [0.1, 0.2, 0.7],\n",
    "])\n",
    "average_precision_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19575c0-8cdf-4feb-b57b-0a88d1bcd1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
